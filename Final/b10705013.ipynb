{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bfab6d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:44:58.551823Z",
     "iopub.status.busy": "2024-12-23T15:44:58.551472Z",
     "iopub.status.idle": "2024-12-23T15:48:52.525273Z",
     "shell.execute_reply": "2024-12-23T15:48:52.524432Z"
    },
    "papermill": {
     "duration": 233.982834,
     "end_time": "2024-12-23T15:48:52.527852",
     "exception": false,
     "start_time": "2024-12-23T15:44:58.545018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.4.0\r\n",
      "Uninstalling torch-2.4.0:\r\n",
      "  Successfully uninstalled torch-2.4.0\r\n",
      "Processing /kaggle/input/logits-processor-zoo/logits_processor_zoo-0.1.0-py3-none-any.whl\r\n",
      "Installing collected packages: logits-processor-zoo\r\n",
      "Successfully installed logits-processor-zoo-0.1.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y torch\n",
    "!pip install -q --no-index --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-vllm vllm\n",
    "!pip install -q -U /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!pip install -q -U /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl\n",
    "!pip install -q --no-deps --no-index /kaggle/input/hf-libraries/sentence-transformers/sentence_transformers-3.1.0-py3-none-any.whl\n",
    "!pip install --no-deps --no-index /kaggle/input/logits-processor-zoo/logits_processor_zoo-0.1.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2967e66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:48:52.540204Z",
     "iopub.status.busy": "2024-12-23T15:48:52.539911Z",
     "iopub.status.idle": "2024-12-23T15:49:01.151593Z",
     "shell.execute_reply": "2024-12-23T15:49:01.150642Z"
    },
    "papermill": {
     "duration": 8.621061,
     "end_time": "2024-12-23T15:49:01.153992",
     "exception": false,
     "start_time": "2024-12-23T15:48:52.532931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers peft accelerate \\\n",
    "    -q -U --no-index --find-links /kaggle/input/lmsys-wheel-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85cbbbfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:49:01.165129Z",
     "iopub.status.busy": "2024-12-23T15:49:01.164820Z",
     "iopub.status.idle": "2024-12-23T15:49:44.638184Z",
     "shell.execute_reply": "2024-12-23T15:49:44.637234Z"
    },
    "papermill": {
     "duration": 43.481748,
     "end_time": "2024-12-23T15:49:44.640707",
     "exception": false,
     "start_time": "2024-12-23T15:49:01.158959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/bitsandbytes0-42-0\r\n",
      "Processing /kaggle/input/bitsandbytes0-42-0/bitsandbytes-0.42.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes==0.42.0) (1.14.1)\r\n",
      "Requirement already satisfied: numpy<2.3,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from scipy->bitsandbytes==0.42.0) (1.26.4)\r\n",
      "Installing collected packages: bitsandbytes\r\n",
      "Successfully installed bitsandbytes-0.42.0\r\n",
      "Looking in links: /kaggle/input/bitsandbytes0-42-0\r\n",
      "Processing /kaggle/input/bitsandbytes0-42-0/optimum-1.21.2-py3-none-any.whl\r\n",
      "Processing /kaggle/input/bitsandbytes0-42-0/coloredlogs-15.0.1-py2.py3-none-any.whl (from optimum==1.21.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum==1.21.2) (1.13.3)\r\n",
      "Processing /kaggle/input/bitsandbytes0-42-0/transformers-4.42.4-py3-none-any.whl (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum==1.21.2)\r\n",
      "Requirement already satisfied: torch>=1.11 in /opt/conda/lib/python3.10/site-packages (from optimum==1.21.2) (2.3.1)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from optimum==1.21.2) (21.3)\r\n",
      "Requirement already satisfied: numpy<2.0 in /opt/conda/lib/python3.10/site-packages (from optimum==1.21.2) (1.26.4)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from optimum==1.21.2) (0.25.1)\r\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from optimum==1.21.2) (3.0.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum==1.21.2) (3.15.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum==1.21.2) (2024.6.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum==1.21.2) (6.0.2)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum==1.21.2) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum==1.21.2) (4.66.4)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum==1.21.2) (4.12.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->optimum==1.21.2) (3.1.2)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.21.2) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.21.2) (3.1.4)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.21.2) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.21.2) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.21.2) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.21.2) (8.9.2.26)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.21.2) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.21.2) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.21.2) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.21.2) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.21.2) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.21.2) (2.20.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.21.2) (12.1.105)\r\n",
      "Requirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.21.2) (2.3.1)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->optimum==1.21.2) (12.6.77)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum==1.21.2) (2024.5.15)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum==1.21.2) (0.4.5)\r\n",
      "Processing /kaggle/input/bitsandbytes0-42-0/tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum==1.21.2)\r\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum==1.21.2) (3.20.3)\r\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum==1.21.2) (0.2.0)\r\n",
      "Processing /kaggle/input/bitsandbytes0-42-0/humanfriendly-10.0-py2.py3-none-any.whl (from coloredlogs->optimum==1.21.2)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.21.2) (16.1.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.21.2) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.21.2) (2.2.2)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.21.2) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.21.2) (0.70.16)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.21.2) (3.9.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum==1.21.2) (1.3.0)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.21.2) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.21.2) (23.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.21.2) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.21.2) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.21.2) (1.9.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.21.2) (4.0.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum==1.21.2) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum==1.21.2) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum==1.21.2) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum==1.21.2) (2024.8.30)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum==1.21.2) (2.1.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum==1.21.2) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum==1.21.2) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum==1.21.2) (2024.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum==1.21.2) (1.16.0)\r\n",
      "Installing collected packages: humanfriendly, coloredlogs, tokenizers, transformers, optimum\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.20.0\r\n",
      "    Uninstalling tokenizers-0.20.0:\r\n",
      "      Successfully uninstalled tokenizers-0.20.0\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.45.1\r\n",
      "    Uninstalling transformers-4.45.1:\r\n",
      "      Successfully uninstalled transformers-4.45.1\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "logits-processor-zoo 0.1.0 requires accelerate<0.27.0,>=0.26.1, but you have accelerate 0.34.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed coloredlogs-15.0.1 humanfriendly-10.0 optimum-1.21.2 tokenizers-0.19.1 transformers-4.42.4\r\n",
      "Looking in links: /kaggle/input/bitsandbytes0-42-0\r\n",
      "Processing /kaggle/input/bitsandbytes0-42-0/auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq==0.7.1) (0.34.2)\r\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from auto-gptq==0.7.1) (3.0.1)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from auto-gptq==0.7.1) (0.2.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from auto-gptq==0.7.1) (1.26.4)\r\n",
      "Processing /kaggle/input/bitsandbytes0-42-0/rouge-1.0.1-py3-none-any.whl (from auto-gptq==0.7.1)\r\n",
      "Processing /kaggle/input/bitsandbytes0-42-0/gekko-1.2.1-py3-none-any.whl (from auto-gptq==0.7.1)\r\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq==0.7.1) (2.3.1)\r\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from auto-gptq==0.7.1) (0.4.5)\r\n",
      "Requirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq==0.7.1) (4.42.4)\r\n",
      "Requirement already satisfied: peft>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq==0.7.1) (0.11.1)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from auto-gptq==0.7.1) (4.66.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq==0.7.1) (21.3)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq==0.7.1) (5.9.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq==0.7.1) (6.0.2)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq==0.7.1) (0.25.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (3.15.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (2024.6.1)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (8.9.2.26)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (2.20.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (12.1.105)\r\n",
      "Requirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq==0.7.1) (2.3.1)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->auto-gptq==0.7.1) (12.6.77)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq==0.7.1) (2024.5.15)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq==0.7.1) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq==0.7.1) (0.19.1)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq==0.7.1) (16.1.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq==0.7.1) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq==0.7.1) (2.2.2)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq==0.7.1) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq==0.7.1) (0.70.16)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq==0.7.1) (3.9.5)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge->auto-gptq==0.7.1) (1.16.0)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq==0.7.1) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq==0.7.1) (23.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq==0.7.1) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq==0.7.1) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq==0.7.1) (1.9.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq==0.7.1) (4.0.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate>=0.26.0->auto-gptq==0.7.1) (3.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq==0.7.1) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq==0.7.1) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq==0.7.1) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq==0.7.1) (2024.8.30)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->auto-gptq==0.7.1) (2.1.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->auto-gptq==0.7.1) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->auto-gptq==0.7.1) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->auto-gptq==0.7.1) (2024.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->auto-gptq==0.7.1) (1.3.0)\r\n",
      "Installing collected packages: rouge, gekko, auto-gptq\r\n",
      "Successfully installed auto-gptq-0.7.1 gekko-1.2.1 rouge-1.0.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-index /kaggle/input/bitsandbytes0-42-0/bitsandbytes-0.42.0-py3-none-any.whl --find-links=/kaggle/input/bitsandbytes0-42-0\n",
    "!pip install --no-index  /kaggle/input/bitsandbytes0-42-0/optimum-1.21.2-py3-none-any.whl --find-links=/kaggle/input/bitsandbytes0-42-0\n",
    "!pip install --no-index  /kaggle/input/bitsandbytes0-42-0/auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --find-links=/kaggle/input/bitsandbytes0-42-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b376ac1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:49:44.656309Z",
     "iopub.status.busy": "2024-12-23T15:49:44.656012Z",
     "iopub.status.idle": "2024-12-23T15:49:53.287197Z",
     "shell.execute_reply": "2024-12-23T15:49:53.286271Z"
    },
    "papermill": {
     "duration": 8.641274,
     "end_time": "2024-12-23T15:49:53.289408",
     "exception": false,
     "start_time": "2024-12-23T15:49:44.648134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "from numpy.linalg import norm\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch import Tensor\n",
    "from tqdm.autonotebook import trange\n",
    "from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "import torch\n",
    "from numpy.linalg import norm\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForMaskedLM, AutoModel, BitsAndBytesConfig\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2874709",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:49:53.304772Z",
     "iopub.status.busy": "2024-12-23T15:49:53.304389Z",
     "iopub.status.idle": "2024-12-23T15:49:53.351501Z",
     "shell.execute_reply": "2024-12-23T15:49:53.350783Z"
    },
    "papermill": {
     "duration": 0.056634,
     "end_time": "2024-12-23T15:49:53.353175",
     "exception": false,
     "start_time": "2024-12-23T15:49:53.296541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")\n",
    "misconception_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n",
    "\n",
    "model_path = \"/kaggle/input/qwen2.5-14/pytorch/default/1\"\n",
    "lora_path='/kaggle/input/qwen14b-it-lora/lora_weights/adapter.bin'\n",
    "\n",
    "device='cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04606f6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:49:53.369151Z",
     "iopub.status.busy": "2024-12-23T15:49:53.368380Z",
     "iopub.status.idle": "2024-12-23T15:49:53.372324Z",
     "shell.execute_reply": "2024-12-23T15:49:53.371523Z"
    },
    "papermill": {
     "duration": 0.013315,
     "end_time": "2024-12-23T15:49:53.373783",
     "exception": false,
     "start_time": "2024-12-23T15:49:53.360468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IS_SUBMISSION = bool(os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88dd33dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:49:53.388996Z",
     "iopub.status.busy": "2024-12-23T15:49:53.388419Z",
     "iopub.status.idle": "2024-12-23T15:49:53.413965Z",
     "shell.execute_reply": "2024-12-23T15:49:53.413265Z"
    },
    "papermill": {
     "duration": 0.034957,
     "end_time": "2024-12-23T15:49:53.415774",
     "exception": false,
     "start_time": "2024-12-23T15:49:53.380817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_text</th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>ConstructName</th>\n",
       "      <th>SubjectName</th>\n",
       "      <th>QuestionText</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>incorrect_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SubjectName: BIDMAS\\n        ConstructName: Us...</td>\n",
       "      <td>1869_B</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>\\( 3 \\times 2+(4-5) \\)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SubjectName: BIDMAS\\n        ConstructName: Us...</td>\n",
       "      <td>1869_C</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>\\( 3 \\times(2+4-5) \\)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SubjectName: BIDMAS\\n        ConstructName: Us...</td>\n",
       "      <td>1869_D</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>Does not need brackets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SubjectName: Simplifying Algebraic Fractions\\n...</td>\n",
       "      <td>1870_A</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>Does not simplify</td>\n",
       "      <td>\\( m+1 \\)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SubjectName: Simplifying Algebraic Fractions\\n...</td>\n",
       "      <td>1870_B</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>Does not simplify</td>\n",
       "      <td>\\( m+2 \\)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          query_text QuestionId_Answer  \\\n",
       "0  SubjectName: BIDMAS\\n        ConstructName: Us...            1869_B   \n",
       "1  SubjectName: BIDMAS\\n        ConstructName: Us...            1869_C   \n",
       "2  SubjectName: BIDMAS\\n        ConstructName: Us...            1869_D   \n",
       "3  SubjectName: Simplifying Algebraic Fractions\\n...            1870_A   \n",
       "4  SubjectName: Simplifying Algebraic Fractions\\n...            1870_B   \n",
       "\n",
       "                                       ConstructName  \\\n",
       "0  Use the order of operations to carry out calcu...   \n",
       "1  Use the order of operations to carry out calcu...   \n",
       "2  Use the order of operations to carry out calcu...   \n",
       "3  Simplify an algebraic fraction by factorising ...   \n",
       "4  Simplify an algebraic fraction by factorising ...   \n",
       "\n",
       "                       SubjectName  \\\n",
       "0                           BIDMAS   \n",
       "1                           BIDMAS   \n",
       "2                           BIDMAS   \n",
       "3  Simplifying Algebraic Fractions   \n",
       "4  Simplifying Algebraic Fractions   \n",
       "\n",
       "                                        QuestionText         correct_answer  \\\n",
       "0  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
       "1  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
       "2  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
       "3  Simplify the following, if possible: \\( \\frac{...      Does not simplify   \n",
       "4  Simplify the following, if possible: \\( \\frac{...      Does not simplify   \n",
       "\n",
       "         incorrect_answer  \n",
       "0  \\( 3 \\times 2+(4-5) \\)  \n",
       "1   \\( 3 \\times(2+4-5) \\)  \n",
       "2  Does not need brackets  \n",
       "3               \\( m+1 \\)  \n",
       "4               \\( m+2 \\)  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "for idx, row in test_df.iterrows():\n",
    "    for option in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "        if option == row.CorrectAnswer:\n",
    "            continue\n",
    "\n",
    "        correct_answer = row[f\"Answer{row.CorrectAnswer}Text\"]\n",
    "\n",
    "        query_text = f\"\"\"\n",
    "        SubjectName: {row['SubjectName']}\n",
    "        ConstructName: {row['ConstructName']}\n",
    "        Question: {row['QuestionText']}\n",
    "        Correct Answer: {correct_answer}\n",
    "        Misconcepte Incorrect answer: {option}.{row[f'Answer{option}Text']}\n",
    "        \"\"\"\n",
    "        rows.append(\n",
    "            {\n",
    "                \"query_text\": query_text.strip(),\n",
    "                \"QuestionId_Answer\": f\"{row.QuestionId}_{option}\",\n",
    "                \"ConstructName\": row.ConstructName,\n",
    "                \"SubjectName\": row.SubjectName,\n",
    "                \"QuestionText\": row.QuestionText,\n",
    "                \"correct_answer\": correct_answer,\n",
    "                \"incorrect_answer\": row[f\"Answer{option}Text\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5646c3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:49:53.433565Z",
     "iopub.status.busy": "2024-12-23T15:49:53.433313Z",
     "iopub.status.idle": "2024-12-23T15:49:53.437643Z",
     "shell.execute_reply": "2024-12-23T15:49:53.436818Z"
    },
    "papermill": {
     "duration": 0.015248,
     "end_time": "2024-12-23T15:49:53.439474",
     "exception": false,
     "start_time": "2024-12-23T15:49:53.424226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 移動張量\n",
    "def batch_to_device(batch, target_device):\n",
    "    for key in batch:\n",
    "        if isinstance(batch[key], Tensor):\n",
    "            batch[key] = batch[key].to(target_device)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ea2b532",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:49:53.455329Z",
     "iopub.status.busy": "2024-12-23T15:49:53.454880Z",
     "iopub.status.idle": "2024-12-23T15:49:53.459324Z",
     "shell.execute_reply": "2024-12-23T15:49:53.458638Z"
    },
    "papermill": {
     "duration": 0.013589,
     "end_time": "2024-12-23T15:49:53.460799",
     "exception": false,
     "start_time": "2024-12-23T15:49:53.447210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0da591a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:49:53.475967Z",
     "iopub.status.busy": "2024-12-23T15:49:53.475549Z",
     "iopub.status.idle": "2024-12-23T15:49:53.479078Z",
     "shell.execute_reply": "2024-12-23T15:49:53.478346Z"
    },
    "papermill": {
     "duration": 0.012847,
     "end_time": "2024-12-23T15:49:53.480669",
     "exception": false,
     "start_time": "2024-12-23T15:49:53.467822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery: {query}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40de9060",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:49:53.495770Z",
     "iopub.status.busy": "2024-12-23T15:49:53.495360Z",
     "iopub.status.idle": "2024-12-23T15:49:53.501846Z",
     "shell.execute_reply": "2024-12-23T15:49:53.501181Z"
    },
    "papermill": {
     "duration": 0.015803,
     "end_time": "2024-12-23T15:49:53.503357",
     "exception": false,
     "start_time": "2024-12-23T15:49:53.487554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference(df, model, tokenizer, device):\n",
    "    batch_size = 16\n",
    "    max_length = 512\n",
    "    sentences = list(df['query_text'].values)\n",
    "\n",
    "    all_embeddings = []\n",
    "    length_sorted_idx = np.argsort([-len(sen) for sen in sentences])\n",
    "    sentences_sorted = [sentences[idx] for idx in length_sorted_idx]\n",
    "    for start_index in trange(0, len(sentences), batch_size, desc=\"Batches\", disable=False):\n",
    "        sentences_batch = sentences_sorted[start_index: start_index + batch_size]\n",
    "        features = tokenizer(sentences_batch, max_length=max_length, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        features = batch_to_device(features, device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**features)\n",
    "            embeddings = last_token_pool(outputs.last_hidden_state, features['attention_mask'])\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, dim=-1)\n",
    "            embeddings = embeddings.detach().cpu().numpy().tolist()\n",
    "        all_embeddings.extend(embeddings)\n",
    "\n",
    "    all_embeddings = [np.array(all_embeddings[idx]).reshape(1, -1) for idx in np.argsort(length_sorted_idx)]\n",
    "\n",
    "    return np.concatenate(all_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe6f8006",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:49:53.518383Z",
     "iopub.status.busy": "2024-12-23T15:49:53.518143Z",
     "iopub.status.idle": "2024-12-23T15:53:45.945776Z",
     "shell.execute_reply": "2024-12-23T15:53:45.944806Z"
    },
    "papermill": {
     "duration": 232.437669,
     "end_time": "2024-12-23T15:53:45.948018",
     "exception": false,
     "start_time": "2024-12-23T15:49:53.510349",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a80f76a77546238971479a9c89f3ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading lora\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(lora_path.replace(\"/adapter.bin\",\"\"))\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "model_base = AutoModel.from_pretrained(model_path, \n",
    "                                  quantization_config=bnb_config, \n",
    "                                  device_map=device,\n",
    "                                  trust_remote_code=True)\n",
    "\n",
    "if lora_path:\n",
    "    print(\"loading lora\")\n",
    "    config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=128,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],\n",
    "        bias=\"none\",\n",
    "        lora_dropout=0.05,  # Conventional\n",
    "        task_type=\"FEATURE_EXTRACTION\",\n",
    "    )\n",
    "    model = get_peft_model(model_base, config)\n",
    "    d = torch.load(lora_path, map_location=model.device)\n",
    "    model.load_state_dict(d, strict=False)\n",
    "    model = model.merge_and_unload()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dbf804f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:53:45.964328Z",
     "iopub.status.busy": "2024-12-23T15:53:45.964048Z",
     "iopub.status.idle": "2024-12-23T15:54:46.326229Z",
     "shell.execute_reply": "2024-12-23T15:54:46.325364Z"
    },
    "papermill": {
     "duration": 60.372264,
     "end_time": "2024-12-23T15:54:46.327933",
     "exception": false,
     "start_time": "2024-12-23T15:53:45.955669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98bc4f39cce04f0ebd25a2c52e7ecc18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a2569bc9874af5a68bd1d3d09736f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((9, 5120), (100, 5120))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_description = 'Given a math question with correct answer and a misconcepted incorrect answer, retrieve the most accurate misconception for the incorrect answer.'\n",
    "df['query_text'] = df['query_text'].apply(lambda x: get_detailed_instruct(task_description, x))\n",
    "\n",
    "V_answer = inference(df, model, tokenizer, device)\n",
    "\n",
    "misconception_df[\"query_text\"] = misconception_df[\"MisconceptionName\"]\n",
    "if not IS_SUBMISSION:\n",
    "    misconception_df = misconception_df[:100]\n",
    "\n",
    "\n",
    "V_misconception = inference(misconception_df, model, tokenizer, device)\n",
    "\n",
    "V_answer.shape,V_misconception.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea0ab0cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:54:46.345589Z",
     "iopub.status.busy": "2024-12-23T15:54:46.345060Z",
     "iopub.status.idle": "2024-12-23T15:54:46.349684Z",
     "shell.execute_reply": "2024-12-23T15:54:46.349048Z"
    },
    "papermill": {
     "duration": 0.014955,
     "end_time": "2024-12-23T15:54:46.351298",
     "exception": false,
     "start_time": "2024-12-23T15:54:46.336343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_matches(V_topic, V_content, n_neighbors=25):\n",
    "    neighbors_model = NearestNeighbors(n_neighbors=n_neighbors, metric='cosine', algorithm=\"brute\", n_jobs=-1)\n",
    "    neighbors_model.fit(V_content)\n",
    "    dists, indices = neighbors_model.kneighbors(V_topic)\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "416a4f3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:54:46.367688Z",
     "iopub.status.busy": "2024-12-23T15:54:46.367439Z",
     "iopub.status.idle": "2024-12-23T15:54:46.410206Z",
     "shell.execute_reply": "2024-12-23T15:54:46.409405Z"
    },
    "papermill": {
     "duration": 0.05279,
     "end_time": "2024-12-23T15:54:46.411746",
     "exception": false,
     "start_time": "2024-12-23T15:54:46.358956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 25)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = get_matches(V_answer, V_misconception, n_neighbors=25)\n",
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5314c3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:54:46.430997Z",
     "iopub.status.busy": "2024-12-23T15:54:46.430280Z",
     "iopub.status.idle": "2024-12-23T15:54:47.055601Z",
     "shell.execute_reply": "2024-12-23T15:54:47.054644Z"
    },
    "papermill": {
     "duration": 0.637019,
     "end_time": "2024-12-23T15:54:47.057750",
     "exception": false,
     "start_time": "2024-12-23T15:54:46.420731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model_base, model, tokenizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1df2bbdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:54:47.075451Z",
     "iopub.status.busy": "2024-12-23T15:54:47.074713Z",
     "iopub.status.idle": "2024-12-23T15:54:47.316087Z",
     "shell.execute_reply": "2024-12-23T15:54:47.315207Z"
    },
    "papermill": {
     "duration": 0.251785,
     "end_time": "2024-12-23T15:54:47.317872",
     "exception": false,
     "start_time": "2024-12-23T15:54:47.066087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save(\"indices.npy\", indices)\n",
    "df.to_parquet(\"df.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2eed8775",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:54:47.334963Z",
     "iopub.status.busy": "2024-12-23T15:54:47.334686Z",
     "iopub.status.idle": "2024-12-23T15:54:47.342770Z",
     "shell.execute_reply": "2024-12-23T15:54:47.341949Z"
    },
    "papermill": {
     "duration": 0.018892,
     "end_time": "2024-12-23T15:54:47.344666",
     "exception": false,
     "start_time": "2024-12-23T15:54:47.325774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_vllm.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_vllm.py\n",
    "\n",
    "import vllm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import PreTrainedTokenizer, AutoTokenizer\n",
    "from typing import List\n",
    "import torch\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "import re\n",
    "from copy import deepcopy\n",
    "\n",
    "import logging\n",
    "from transformers.utils.logging import disable_progress_bar\n",
    "\n",
    "# 調整 vLLM 的日誌\n",
    "disable_progress_bar()\n",
    "logging.getLogger(\"vllm\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"transformers\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"torch\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"vllm.triton_utils\").setLevel(logging.CRITICAL)\n",
    "\n",
    "# load anything\n",
    "model_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "df = pd.read_parquet(\"df.parquet\")\n",
    "misconception_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n",
    "indices = np.load(\"indices.npy\")\n",
    "\n",
    "print(\"indices :\")\n",
    "print(indices)\n",
    "\n",
    "# prompt\n",
    "PROMPT  = \"\"\"Here is a question about {ConstructName}({SubjectName}).\n",
    "Question: {Question}\n",
    "Correct Answer: {CorrectAnswer}\n",
    "Incorrect Answer: {IncorrectAnswer}\n",
    "\n",
    "You are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer with the Question.\n",
    "Answer concisely what misconception it is to lead to getting the incorrect answer.\n",
    "Pick the correct misconception number from the below:\n",
    "\n",
    "{Retrival}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert Mathematics teacher specialized in diagnosing and addressing students' conceptual misunderstandings. Your job is to analyze their incorrect answers, trace the reasoning error, and identify the precise underlying misconception from the provided options.\"\"\"\n",
    "\n",
    "\n",
    "# preprocess function\n",
    "def preprocess_text(x):\n",
    "    x = re.sub(\"http\\w+\", '',x)   # Delete URL\n",
    "    x = re.sub(r\"\\.+\", \".\", x)    # Replace consecutive commas and periods with one comma and period character\n",
    "    x = re.sub(r\"\\,+\", \",\", x)\n",
    "    x = re.sub(r\"\\\\\\(\", \" \", x)\n",
    "    x = re.sub(r\"\\\\\\)\", \" \", x)\n",
    "    x = re.sub(r\"[ ]{1,}\", \" \", x)\n",
    "    x = x.strip()                 # Remove empty characters at the beginning and end\n",
    "    return x\n",
    "\n",
    "# chat message\n",
    "def apply_template(row, tokenizer):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": preprocess_text(\n",
    "                PROMPT.format(\n",
    "                    ConstructName=row[\"ConstructName\"],\n",
    "                    SubjectName=row[\"SubjectName\"],\n",
    "                    Question=row[\"QuestionText\"],\n",
    "                    IncorrectAnswer=row[f\"incorrect_answer\"],\n",
    "                    CorrectAnswer=row[f\"correct_answer\"],\n",
    "                    Retrival=row[f\"retrieval\"]\n",
    "                )\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    # print(messages[1][\"content\"])\n",
    "    return text\n",
    "\n",
    "# vllm config\n",
    "llm = vllm.LLM(\n",
    "    model_path,\n",
    "    quantization=\"awq\",\n",
    "    tensor_parallel_size=2,\n",
    "    gpu_memory_utilization=0.90, \n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\",\n",
    "    enforce_eager=True,\n",
    "    max_model_len=5120,\n",
    "    disable_log_stats=True\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "# process candidate\n",
    "def get_candidates(c_indices):\n",
    "    candidates = []\n",
    "\n",
    "    mis_names = misconception_df[\"MisconceptionName\"].values\n",
    "    for ix in c_indices:\n",
    "        c_names = []\n",
    "        for i, name in enumerate(mis_names[ix]):\n",
    "            c_names.append(f\"{i+1}. {name}\")\n",
    "\n",
    "        candidates.append(\"\\n\".join(c_names))\n",
    "        \n",
    "    return candidates\n",
    "\n",
    "# spilt group\n",
    "def split_and_concatenate(array1, group_size):\n",
    "    array1 = array1[:,:-1]\n",
    "    m, n = array1.shape\n",
    "    \n",
    "    num_groups = -(-n // group_size)\n",
    "    results = []\n",
    "\n",
    "    # 倒序處理 array1 的分組\n",
    "    for i in range(num_groups):\n",
    "        start_col = n - (i + 1) * group_size\n",
    "        end_col = n - i * group_size\n",
    "\n",
    "        if start_col < 0:\n",
    "            start_col = 0\n",
    "\n",
    "        group = array1[:, start_col:end_col]\n",
    "\n",
    "        results.append(group)  # 插入到最前方以保留順序\n",
    "\n",
    "    return results\n",
    "\n",
    "# filter survivors\n",
    "def filter(remain_indices, survivors):\n",
    "\n",
    "    survivors = survivors.flatten()\n",
    "    \n",
    "    mask = remain_indices != survivors[:, np.newaxis]\n",
    "\n",
    "    filtered_remain_indices = [row[mask[i]] for i, row in enumerate(remain_indices)]\n",
    "    \n",
    "    filtered_remain_indices = np.array(filtered_remain_indices)\n",
    "    \n",
    "    return filtered_remain_indices\n",
    "\n",
    "\n",
    "# main function\n",
    "remain_indices = deepcopy(indices)\n",
    "all_survivors = []\n",
    "groups = 8\n",
    "\n",
    "for i in range(3):\n",
    "    survivors = remain_indices[:, -1:]\n",
    "    c_indices = split_and_concatenate(remain_indices, groups)\n",
    "\n",
    "    for j in range(len(c_indices)):\n",
    "        group = c_indices[j]\n",
    "        group = np.hstack((group, survivors))\n",
    "        choices = [str(i) for i in range(1, len(group[0]) + 1)]\n",
    "\n",
    "        print(f\"{i}:{j} - {group}\")\n",
    "\n",
    "\n",
    "        df[\"retrieval\"] = get_candidates(group)\n",
    "        df[\"text\"] = df.apply(lambda row: apply_template(row, tokenizer), axis=1)\n",
    "\n",
    "        # print(df[\"text\"].values[0])\n",
    "        # print()\n",
    "\n",
    "        responses = llm.generate(\n",
    "            df[\"text\"].values,\n",
    "            vllm.SamplingParams(\n",
    "                n=1,  # Number of output sequences to return for each prompt.\n",
    "                top_k=1,  # Float that controls the cumulative probability of the top tokens to consider.\n",
    "                temperature=0,  # randomness of the sampling\n",
    "                seed=777, # Seed for reprodicibility\n",
    "                skip_special_tokens=False,  # Whether to skip special tokens in the output.\n",
    "                max_tokens=1,  # Maximum number of tokens to generate per output sequence.\n",
    "                logits_processors=[MultipleChoiceLogitsProcessor(tokenizer, choices=choices)]\n",
    "            ),\n",
    "            use_tqdm=False\n",
    "        )\n",
    "        \n",
    "        responses = [x.outputs[0].text for x in responses]\n",
    "        df[\"response\"] = responses\n",
    "\n",
    "        llm_choices = df[\"response\"].astype(int).values - 1\n",
    "        print(llm_choices)\n",
    "\n",
    "        survivors = np.array([cix[best] for best, cix in zip(llm_choices, group)]).reshape(-1, 1)\n",
    "        print(f\"{i}:{j} - {survivors}\")\n",
    "\n",
    "    remain_indices = filter(remain_indices, survivors)\n",
    "    all_survivors.append(survivors)\n",
    "    \n",
    "all_survivors = np.hstack(all_survivors)\n",
    "print(\"all survivors\")\n",
    "print(all_survivors)\n",
    "\n",
    "# save file\n",
    "results = []\n",
    "\n",
    "for i in range(indices.shape[0]):\n",
    "    temp = [str(x) for x in all_survivors[i]] + [\n",
    "        str(x) for x in indices[i] if x not in all_survivors[i]\n",
    "    ]\n",
    "    temp = temp[:25]\n",
    "    results.append(\" \".join(temp))\n",
    "\n",
    "df[\"MisconceptionId\"] = results\n",
    "df.to_csv(\"submission.csv\", columns=[\"QuestionId_Answer\", \"MisconceptionId\"], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30a55114",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:54:47.361631Z",
     "iopub.status.busy": "2024-12-23T15:54:47.360942Z",
     "iopub.status.idle": "2024-12-23T15:58:10.752607Z",
     "shell.execute_reply": "2024-12-23T15:58:10.751612Z"
    },
    "papermill": {
     "duration": 203.402417,
     "end_time": "2024-12-23T15:58:10.754770",
     "exception": false,
     "start_time": "2024-12-23T15:54:47.352353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices :\r\n",
      "[[15 55 77 95 82 96 34  4 13 75 74 17 29 42 73 14 38 84 60  8 24 89 30 25\r\n",
      "  52]\r\n",
      " [77 15 55 82 13 74 14 25 29 84 52 30 38 73 98  0 42 24 68 51 95  9 71 49\r\n",
      "  23]\r\n",
      " [15 77 74 55 95 14 13 24 29 73 82  0 25 52  4 96 38 17  3 84 90 89 80 64\r\n",
      "  34]\r\n",
      " [59 80 91  3 29 55 46 79 82 34 74  1 94  6 19  0 44 24 96 58 32 20 49 25\r\n",
      "  98]\r\n",
      " [59 80 91  3 79 46 29 74 55 34 58 94  6 27 14 24 32 73 44 49 82 65 68  1\r\n",
      "  15]\r\n",
      " [59 80 91  3 29 46 79 55 82 74  1 24 98  6 34 25  0 49 14 73 68 65 76 58\r\n",
      "  94]\r\n",
      " [14 25 82 38 30 49 74 23 24 36  0 56 84 55 37 63 97 85 98 62 79 18 46  7\r\n",
      "  80]\r\n",
      " [25 82 14 49 38  0 74 62 85 56 24 30 55 36 84 23 97 18 37  7  8  3 40 75\r\n",
      "  31]\r\n",
      " [ 0 23 56 14 25 18 62 74 84 24  3 38 85 82 49 87 97 31 95 55 79 89 12 63\r\n",
      "   7]]\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:19<01:17, 19.49s/it]\r\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:40<01:01, 20.34s/it]\r\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [01:02<00:42, 21.10s/it]\r\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [01:25<00:22, 22.06s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [01:48<00:00, 22.16s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [01:48<00:00, 21.66s/it]\r\n",
      "\r\n",
      "0:0 - [[38 84 60  8 24 89 30 25 52]\r\n",
      " [42 24 68 51 95  9 71 49 23]\r\n",
      " [38 17  3 84 90 89 80 64 34]\r\n",
      " [44 24 96 58 32 20 49 25 98]\r\n",
      " [32 73 44 49 82 65 68  1 15]\r\n",
      " [ 0 49 14 73 68 65 76 58 94]\r\n",
      " [97 85 98 62 79 18 46  7 80]\r\n",
      " [97 18 37  7  8  3 40 75 31]\r\n",
      " [97 31 95 55 79 89 12 63  7]]\r\n",
      "[3 0 0 0 0 3 3 0 0]\r\n",
      "0:0 - [[ 8]\r\n",
      " [42]\r\n",
      " [38]\r\n",
      " [44]\r\n",
      " [32]\r\n",
      " [73]\r\n",
      " [62]\r\n",
      " [97]\r\n",
      " [97]]\r\n",
      "0:1 - [[13 75 74 17 29 42 73 14  8]\r\n",
      " [29 84 52 30 38 73 98  0 42]\r\n",
      " [29 73 82  0 25 52  4 96 38]\r\n",
      " [82 34 74  1 94  6 19  0 44]\r\n",
      " [55 34 58 94  6 27 14 24 32]\r\n",
      " [82 74  1 24 98  6 34 25 73]\r\n",
      " [24 36  0 56 84 55 37 63 62]\r\n",
      " [85 56 24 30 55 36 84 23 97]\r\n",
      " [84 24  3 38 85 82 49 87 97]]\r\n",
      "[2 3 6 0 2 5 3 2 3]\r\n",
      "0:1 - [[74]\r\n",
      " [30]\r\n",
      " [ 4]\r\n",
      " [82]\r\n",
      " [58]\r\n",
      " [ 6]\r\n",
      " [56]\r\n",
      " [24]\r\n",
      " [38]]\r\n",
      "0:2 - [[15 55 77 95 82 96 34  4 74]\r\n",
      " [77 15 55 82 13 74 14 25 30]\r\n",
      " [15 77 74 55 95 14 13 24  4]\r\n",
      " [59 80 91  3 29 55 46 79 82]\r\n",
      " [59 80 91  3 79 46 29 74 58]\r\n",
      " [59 80 91  3 29 46 79 55  6]\r\n",
      " [14 25 82 38 30 49 74 23 56]\r\n",
      " [25 82 14 49 38  0 74 62 24]\r\n",
      " [ 0 23 56 14 25 18 62 74 38]]\r\n",
      "[8 1 0 0 0 0 3 0 3]\r\n",
      "0:2 - [[74]\r\n",
      " [15]\r\n",
      " [15]\r\n",
      " [59]\r\n",
      " [59]\r\n",
      " [59]\r\n",
      " [38]\r\n",
      " [25]\r\n",
      " [14]]\r\n",
      "1:0 - [[38 84 60  8 24 89 30 25 52]\r\n",
      " [42 24 68 51 95  9 71 49 23]\r\n",
      " [38 17  3 84 90 89 80 64 34]\r\n",
      " [44 24 96 58 32 20 49 25 98]\r\n",
      " [32 73 44 49 82 65 68  1 15]\r\n",
      " [ 0 49 14 73 68 65 76 58 94]\r\n",
      " [97 85 98 62 79 18 46  7 80]\r\n",
      " [97 18 37  7  8  3 40 75 31]\r\n",
      " [97 31 95 55 79 89 12 63  7]]\r\n",
      "[3 0 0 0 0 3 3 0 0]\r\n",
      "1:0 - [[ 8]\r\n",
      " [42]\r\n",
      " [38]\r\n",
      " [44]\r\n",
      " [32]\r\n",
      " [73]\r\n",
      " [62]\r\n",
      " [97]\r\n",
      " [97]]\r\n",
      "1:1 - [[ 4 13 75 17 29 42 73 14  8]\r\n",
      " [29 84 52 30 38 73 98  0 42]\r\n",
      " [29 73 82  0 25 52  4 96 38]\r\n",
      " [82 34 74  1 94  6 19  0 44]\r\n",
      " [55 34 58 94  6 27 14 24 32]\r\n",
      " [82 74  1 24 98  6 34 25 73]\r\n",
      " [24 36  0 56 84 55 37 63 62]\r\n",
      " [85 56 24 30 55 36 84 23 97]\r\n",
      " [84 24  3 38 85 82 49 87 97]]\r\n",
      "[5 3 6 0 2 5 3 2 3]\r\n",
      "1:1 - [[42]\r\n",
      " [30]\r\n",
      " [ 4]\r\n",
      " [82]\r\n",
      " [58]\r\n",
      " [ 6]\r\n",
      " [56]\r\n",
      " [24]\r\n",
      " [38]]\r\n",
      "1:2 - [[15 55 77 95 82 96 34 42]\r\n",
      " [77 55 82 13 74 14 25 30]\r\n",
      " [77 74 55 95 14 13 24  4]\r\n",
      " [80 91  3 29 55 46 79 82]\r\n",
      " [80 91  3 79 46 29 74 58]\r\n",
      " [80 91  3 29 46 79 55  6]\r\n",
      " [14 25 82 30 49 74 23 56]\r\n",
      " [82 14 49 38  0 74 62 24]\r\n",
      " [ 0 23 56 25 18 62 74 38]]\r\n",
      "[2 5 0 2 3 2 3 3 3]\r\n",
      "1:2 - [[77]\r\n",
      " [14]\r\n",
      " [77]\r\n",
      " [ 3]\r\n",
      " [79]\r\n",
      " [ 3]\r\n",
      " [30]\r\n",
      " [38]\r\n",
      " [25]]\r\n",
      "2:0 - [[38 84 60  8 24 89 30 25 52]\r\n",
      " [42 24 68 51 95  9 71 49 23]\r\n",
      " [38 17  3 84 90 89 80 64 34]\r\n",
      " [44 24 96 58 32 20 49 25 98]\r\n",
      " [32 73 44 49 82 65 68  1 15]\r\n",
      " [ 0 49 14 73 68 65 76 58 94]\r\n",
      " [97 85 98 62 79 18 46  7 80]\r\n",
      " [97 18 37  7  8  3 40 75 31]\r\n",
      " [97 31 95 55 79 89 12 63  7]]\r\n",
      "[3 0 0 0 0 3 3 0 0]\r\n",
      "2:0 - [[ 8]\r\n",
      " [42]\r\n",
      " [38]\r\n",
      " [44]\r\n",
      " [32]\r\n",
      " [73]\r\n",
      " [62]\r\n",
      " [97]\r\n",
      " [97]]\r\n",
      "2:1 - [[ 4 13 75 17 29 42 73 14  8]\r\n",
      " [29 84 52 30 38 73 98  0 42]\r\n",
      " [29 73 82  0 25 52  4 96 38]\r\n",
      " [82 34 74  1 94  6 19  0 44]\r\n",
      " [55 34 58 94  6 27 14 24 32]\r\n",
      " [82 74  1 24 98  6 34 25 73]\r\n",
      " [24 36  0 56 84 55 37 63 62]\r\n",
      " [85 56 24 30 55 36 84 23 97]\r\n",
      " [84 24  3 38 85 82 49 87 97]]\r\n",
      "[5 3 6 0 2 5 3 2 3]\r\n",
      "2:1 - [[42]\r\n",
      " [30]\r\n",
      " [ 4]\r\n",
      " [82]\r\n",
      " [58]\r\n",
      " [ 6]\r\n",
      " [56]\r\n",
      " [24]\r\n",
      " [38]]\r\n",
      "2:2 - [[15 55 95 82 96 34 42]\r\n",
      " [77 55 82 13 74 25 30]\r\n",
      " [74 55 95 14 13 24  4]\r\n",
      " [80 91 29 55 46 79 82]\r\n",
      " [80 91  3 46 29 74 58]\r\n",
      " [80 91 29 46 79 55  6]\r\n",
      " [14 25 82 49 74 23 56]\r\n",
      " [82 14 49  0 74 62 24]\r\n",
      " [ 0 23 56 18 62 74 38]]\r\n",
      "[0 3 3 2 3 4 5 2 3]\r\n",
      "2:2 - [[15]\r\n",
      " [13]\r\n",
      " [14]\r\n",
      " [29]\r\n",
      " [46]\r\n",
      " [79]\r\n",
      " [23]\r\n",
      " [49]\r\n",
      " [18]]\r\n",
      "all survivors\r\n",
      "[[74 77 15]\r\n",
      " [15 14 13]\r\n",
      " [15 77 14]\r\n",
      " [59  3 29]\r\n",
      " [59 79 46]\r\n",
      " [59  3 79]\r\n",
      " [38 30 23]\r\n",
      " [25 38 49]\r\n",
      " [14 25 18]]\r\n"
     ]
    }
   ],
   "source": [
    "!export NCCL_DEBUG=CRITICAL\n",
    "!export LOGLEVEL=CRITICAL\n",
    "!export TOKENIZERS_PARALLELISM=false\n",
    "!python run_vllm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0d0a803",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:58:10.775423Z",
     "iopub.status.busy": "2024-12-23T15:58:10.774571Z",
     "iopub.status.idle": "2024-12-23T15:58:10.788324Z",
     "shell.execute_reply": "2024-12-23T15:58:10.787528Z"
    },
    "papermill": {
     "duration": 0.026219,
     "end_time": "2024-12-23T15:58:10.790205",
     "exception": false,
     "start_time": "2024-12-23T15:58:10.763986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>MisconceptionId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869_B</td>\n",
       "      <td>74 77 15 55 95 82 96 34 4 13 75 17 29 42 73 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869_C</td>\n",
       "      <td>15 14 13 77 55 82 74 25 29 84 52 30 38 73 98 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869_D</td>\n",
       "      <td>15 77 14 74 55 95 13 24 29 73 82 0 25 52 4 96 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870_A</td>\n",
       "      <td>59 3 29 80 91 55 46 79 82 34 74 1 94 6 19 0 44...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870_B</td>\n",
       "      <td>59 79 46 80 91 3 29 74 55 34 58 94 6 27 14 24 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1870_C</td>\n",
       "      <td>59 3 79 80 91 29 46 55 82 74 1 24 98 6 34 25 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1871_A</td>\n",
       "      <td>38 30 23 14 25 82 49 74 24 36 0 56 84 55 37 63...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1871_C</td>\n",
       "      <td>25 38 49 82 14 0 74 62 85 56 24 30 55 36 84 23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1871_D</td>\n",
       "      <td>14 25 18 0 23 56 62 74 84 24 3 38 85 82 49 87 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QuestionId_Answer                                    MisconceptionId\n",
       "0            1869_B  74 77 15 55 95 82 96 34 4 13 75 17 29 42 73 14...\n",
       "1            1869_C  15 14 13 77 55 82 74 25 29 84 52 30 38 73 98 0...\n",
       "2            1869_D  15 77 14 74 55 95 13 24 29 73 82 0 25 52 4 96 ...\n",
       "3            1870_A  59 3 29 80 91 55 46 79 82 34 74 1 94 6 19 0 44...\n",
       "4            1870_B  59 79 46 80 91 3 29 74 55 34 58 94 6 27 14 24 ...\n",
       "5            1870_C  59 3 79 80 91 29 46 55 82 74 1 24 98 6 34 25 0...\n",
       "6            1871_A  38 30 23 14 25 82 49 74 24 36 0 56 84 55 37 63...\n",
       "7            1871_C  25 38 49 82 14 0 74 62 85 56 24 30 55 36 84 23...\n",
       "8            1871_D  14 25 18 0 23 56 62 74 84 24 3 38 85 82 49 87 ..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b06fb450",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:58:10.809592Z",
     "iopub.status.busy": "2024-12-23T15:58:10.809335Z",
     "iopub.status.idle": "2024-12-23T15:58:10.816073Z",
     "shell.execute_reply": "2024-12-23T15:58:10.815357Z"
    },
    "papermill": {
     "duration": 0.018458,
     "end_time": "2024-12-23T15:58:10.817616",
     "exception": false,
     "start_time": "2024-12-23T15:58:10.799158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%writefile run_vllm2.py\n",
    "\n",
    "# import vllm\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from transformers import PreTrainedTokenizer, AutoTokenizer\n",
    "# from typing import List\n",
    "# import torch\n",
    "# from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "# import re\n",
    "\n",
    "# import logging\n",
    "# from transformers.utils.logging import disable_progress_bar\n",
    "\n",
    "# # 調整 vLLM 的日誌\n",
    "# disable_progress_bar()\n",
    "# logging.getLogger(\"vllm\").setLevel(logging.CRITICAL)\n",
    "# logging.getLogger(\"transformers\").setLevel(logging.CRITICAL)\n",
    "# logging.getLogger(\"torch\").setLevel(logging.CRITICAL)\n",
    "# logging.getLogger(\"vllm.triton_utils\").setLevel(logging.CRITICAL)\n",
    "\n",
    "\n",
    "# model_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "# def preprocess_text(x):\n",
    "#     x = re.sub(\"http\\w+\", '',x)   # Delete URL\n",
    "#     x = re.sub(r\"\\.+\", \".\", x)    # Replace consecutive commas and periods with one comma and period character\n",
    "#     x = re.sub(r\"\\,+\", \",\", x)\n",
    "#     x = re.sub(r\"\\\\\\(\", \" \", x)\n",
    "#     x = re.sub(r\"\\\\\\)\", \" \", x)\n",
    "#     x = re.sub(r\"[ ]{1,}\", \" \", x)\n",
    "#     x = x.strip()                 # Remove empty characters at the beginning and end\n",
    "#     return x\n",
    "\n",
    "# PROMPT  = \"\"\"Here is a question about {ConstructName}({SubjectName}).\n",
    "# Question: {Question}\n",
    "# Correct Answer: {CorrectAnswer}\n",
    "# Incorrect Answer: {IncorrectAnswer}\n",
    "\n",
    "# You are a Mathematics teacher. Your task is to reason and identify the misconception behind the Incorrect Answer with the Question.\n",
    "# Answer concisely what misconception it is to lead to getting the incorrect answer.\n",
    "# Pick the correct misconception number from the below:\n",
    "\n",
    "# {Retrival}\n",
    "# \"\"\"\n",
    "# # just directly give your answers.\n",
    "\n",
    "# def apply_template(row, tokenizer):\n",
    "#     messages = [\n",
    "#         {\n",
    "#             \"role\": \"user\", \n",
    "#             \"content\": preprocess_text(\n",
    "#                 PROMPT.format(\n",
    "#                     ConstructName=row[\"ConstructName\"],\n",
    "#                     SubjectName=row[\"SubjectName\"],\n",
    "#                     Question=row[\"QuestionText\"],\n",
    "#                     IncorrectAnswer=row[f\"incorrect_answer\"],\n",
    "#                     CorrectAnswer=row[f\"correct_answer\"],\n",
    "#                     Retrival=row[f\"retrieval\"]\n",
    "#                 )\n",
    "#             )\n",
    "#         }\n",
    "#     ]\n",
    "#     text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "#     return text\n",
    "\n",
    "\n",
    "# misconception_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n",
    "\n",
    "# df = pd.read_parquet(\"df.parquet\")\n",
    "# indices = np.load(\"indices.npy\")\n",
    "\n",
    "# print(indices)\n",
    "\n",
    "# model_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "\n",
    "# llm = vllm.LLM(\n",
    "#     model_path,\n",
    "#     quantization=\"awq\",\n",
    "#     tensor_parallel_size=2,\n",
    "#     gpu_memory_utilization=0.90, \n",
    "#     trust_remote_code=True,\n",
    "#     dtype=\"half\", \n",
    "#     enforce_eager=True,\n",
    "#     max_model_len=5120,\n",
    "#     disable_log_stats=True\n",
    "# )\n",
    "# tokenizer = llm.get_tokenizer()\n",
    "\n",
    "\n",
    "# def get_candidates(c_indices):\n",
    "#     candidates = []\n",
    "\n",
    "#     mis_names = misconception_df[\"MisconceptionName\"].values\n",
    "#     for ix in c_indices:\n",
    "#         c_names = []\n",
    "#         for i, name in enumerate(mis_names[ix]):\n",
    "#             c_names.append(f\"{i+1}. {name}\")\n",
    "\n",
    "#         candidates.append(\"\\n\".join(c_names))\n",
    "        \n",
    "#     return candidates\n",
    "\n",
    "# survivors = indices[:, -1:]\n",
    "\n",
    "# for i in range(3):\n",
    "#     c_indices = np.concatenate([indices[:, -8*(i+1)-1:-8*i-1], survivors], axis=1)\n",
    "\n",
    "#     print(f\"{i} {c_indices}\")\n",
    "    \n",
    "#     df[\"retrieval\"] = get_candidates(c_indices)\n",
    "#     df[\"text\"] = df.apply(lambda row: apply_template(row, tokenizer), axis=1)\n",
    "    \n",
    "#     # print(\"Example:\")\n",
    "#     # print(df[\"text\"].values[0])\n",
    "#     # print()\n",
    "    \n",
    "#     responses = llm.generate(\n",
    "#         df[\"text\"].values,\n",
    "#         vllm.SamplingParams(\n",
    "#             n=1,  # Number of output sequences to return for each prompt.\n",
    "#             top_k=1,  # Float that controls the cumulative probability of the top tokens to consider.\n",
    "#             temperature=0,  # randomness of the sampling\n",
    "#             seed=777, # Seed for reprodicibility\n",
    "#             skip_special_tokens=False,  # Whether to skip special tokens in the output.\n",
    "#             max_tokens=1,  # Maximum number of tokens to generate per output sequence.\n",
    "#             logits_processors=[MultipleChoiceLogitsProcessor(tokenizer, choices=[\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"])]\n",
    "#         ),\n",
    "#         use_tqdm=True\n",
    "#     )\n",
    "    \n",
    "#     responses = [x.outputs[0].text for x in responses]\n",
    "#     df[\"response\"] = responses\n",
    "    \n",
    "    \n",
    "#     llm_choices = df[\"response\"].astype(int).values - 1\n",
    "    \n",
    "#     survivors = np.array([cix[best] for best, cix in zip(llm_choices, c_indices)]).reshape(-1, 1)\n",
    "#     print(survivors)\n",
    "\n",
    "\n",
    "\n",
    "# results = []\n",
    "\n",
    "# for i in range(indices.shape[0]):\n",
    "#     ix = indices[i]\n",
    "#     llm_choice = survivors[i, 0]\n",
    "    \n",
    "#     results.append(\" \".join([str(llm_choice)] + [str(x) for x in ix if x != llm_choice]))\n",
    "\n",
    "\n",
    "# df[\"MisconceptionId\"] = results\n",
    "# df.to_csv(\"submission2.csv\", columns=[\"QuestionId_Answer\", \"MisconceptionId\"], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b42c1f19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:58:10.836336Z",
     "iopub.status.busy": "2024-12-23T15:58:10.836084Z",
     "iopub.status.idle": "2024-12-23T15:58:10.839211Z",
     "shell.execute_reply": "2024-12-23T15:58:10.838501Z"
    },
    "papermill": {
     "duration": 0.014355,
     "end_time": "2024-12-23T15:58:10.840791",
     "exception": false,
     "start_time": "2024-12-23T15:58:10.826436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !export NCCL_DEBUG=CRITICAL\n",
    "# !export LOGLEVEL=CRITICAL\n",
    "# !export TOKENIZERS_PARALLELISM=false\n",
    "# !python run_vllm2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13b65d67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T15:58:10.859390Z",
     "iopub.status.busy": "2024-12-23T15:58:10.859139Z",
     "iopub.status.idle": "2024-12-23T15:58:10.862424Z",
     "shell.execute_reply": "2024-12-23T15:58:10.861689Z"
    },
    "papermill": {
     "duration": 0.014434,
     "end_time": "2024-12-23T15:58:10.863968",
     "exception": false,
     "start_time": "2024-12-23T15:58:10.849534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pd.read_csv(\"submission2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9738540,
     "sourceId": 82695,
     "sourceType": "competition"
    },
    {
     "datasetId": 4871830,
     "sourceId": 8218776,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5297895,
     "sourceId": 8897601,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5251603,
     "sourceId": 9094368,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5920031,
     "sourceId": 9688062,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5957531,
     "sourceId": 9734430,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6117312,
     "sourceId": 9948011,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6135443,
     "sourceId": 9972502,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4581967,
     "sourceId": 10270933,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 200567623,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 123481,
     "modelInstanceId": 99392,
     "sourceId": 118192,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 164048,
     "modelInstanceId": 141580,
     "sourceId": 166386,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 165390,
     "modelInstanceId": 142811,
     "sourceId": 167864,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 799.478349,
   "end_time": "2024-12-23T15:58:14.313999",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-23T15:44:54.835650",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0f36f9e2f69f493eb27c6d8a18dfda8e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b97f5dcb989442a9a4abcd9c71384a34",
       "placeholder": "​",
       "style": "IPY_MODEL_19a5ce73a10043da8dbc1e44e6fa4470",
       "value": "Batches: 100%"
      }
     },
     "1742d5a5337d46138b4d672b5b3600b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3992642f46414ca8b2fb022a5aeda459",
       "max": 8.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6eaaf656bf1e4b61926b5ade99ff5ba7",
       "value": 8.0
      }
     },
     "19a5ce73a10043da8dbc1e44e6fa4470": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1bf7013514384c5a89542bd682652f5c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1f434e17ab4e47dfa586eb9e450c4d34": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "26a80f76a77546238971479a9c89f3ab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4e4a7b8d6b314565a06e777b85472e8c",
        "IPY_MODEL_1742d5a5337d46138b4d672b5b3600b8",
        "IPY_MODEL_88e169d1be0d42cb9ba7328a983b7dd8"
       ],
       "layout": "IPY_MODEL_8207451a79d74373a535695011c421de"
      }
     },
     "28766529e1814f989d1a339debb51087": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3440fb35cfde44c5ad1406081f45597b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "34a2569bc9874af5a68bd1d3d09736f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0f36f9e2f69f493eb27c6d8a18dfda8e",
        "IPY_MODEL_9be22baba31a44508e194f698e16b656",
        "IPY_MODEL_9f881fec23d64058b01c3c8e3bbfcf6e"
       ],
       "layout": "IPY_MODEL_1bf7013514384c5a89542bd682652f5c"
      }
     },
     "3992642f46414ca8b2fb022a5aeda459": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4451660910754966aa3d0918b7e50bfb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4bdadb57ce0f42c3b6c982d7a91f0c9e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_28766529e1814f989d1a339debb51087",
       "placeholder": "​",
       "style": "IPY_MODEL_6b234ae4e91645308afa9b5e1dda4efa",
       "value": "Batches: 100%"
      }
     },
     "4e4a7b8d6b314565a06e777b85472e8c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a6658f7ded9644f5b9c12b4ce4b1b6ac",
       "placeholder": "​",
       "style": "IPY_MODEL_c3d707333a0741b3abcbdbdf84bad73b",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "5c3aea4262d84fee9aa296ac5172aa7f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6b234ae4e91645308afa9b5e1dda4efa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6eaaf656bf1e4b61926b5ade99ff5ba7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "8011bce9b8dc4a97aac1e5783462eb19": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8181751d210046a2b76b343797085af6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8207451a79d74373a535695011c421de": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "88e169d1be0d42cb9ba7328a983b7dd8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5c3aea4262d84fee9aa296ac5172aa7f",
       "placeholder": "​",
       "style": "IPY_MODEL_b84128a323944c0c9b04d38512b8a5cd",
       "value": " 8/8 [02:26&lt;00:00, 14.55s/it]"
      }
     },
     "8cbbc877eef647fa8805b5e858861b8d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "98bc4f39cce04f0ebd25a2c52e7ecc18": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4bdadb57ce0f42c3b6c982d7a91f0c9e",
        "IPY_MODEL_c4d0e95c34c34f2eae1d3ce06af238d0",
        "IPY_MODEL_f397c389dfd64da5835d5ef563eb433a"
       ],
       "layout": "IPY_MODEL_1f434e17ab4e47dfa586eb9e450c4d34"
      }
     },
     "9be22baba31a44508e194f698e16b656": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8011bce9b8dc4a97aac1e5783462eb19",
       "max": 7.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ec06136dff79461197c448b6db6c5fe0",
       "value": 7.0
      }
     },
     "9f881fec23d64058b01c3c8e3bbfcf6e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_df2de3624dd34e389abad7a7fbaa6ba1",
       "placeholder": "​",
       "style": "IPY_MODEL_4451660910754966aa3d0918b7e50bfb",
       "value": " 7/7 [00:27&lt;00:00,  3.06s/it]"
      }
     },
     "a6658f7ded9644f5b9c12b4ce4b1b6ac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b84128a323944c0c9b04d38512b8a5cd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b97f5dcb989442a9a4abcd9c71384a34": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c3d707333a0741b3abcbdbdf84bad73b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c4d0e95c34c34f2eae1d3ce06af238d0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8181751d210046a2b76b343797085af6",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d36687ad5e5140a383e9552945278c18",
       "value": 1.0
      }
     },
     "d36687ad5e5140a383e9552945278c18": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "df2de3624dd34e389abad7a7fbaa6ba1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ec06136dff79461197c448b6db6c5fe0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f397c389dfd64da5835d5ef563eb433a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8cbbc877eef647fa8805b5e858861b8d",
       "placeholder": "​",
       "style": "IPY_MODEL_3440fb35cfde44c5ad1406081f45597b",
       "value": " 1/1 [00:32&lt;00:00, 32.98s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
